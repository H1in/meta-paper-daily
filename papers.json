{"source-free": {"Semantics, Distortion, and Style Matter: Towards Source-free UDA for Panoramic Segmentation": "|**2024-3-22**|**Semantics, Distortion, and Style Matter: Towards Source-free UDA for Panoramic Segmentation**|Xu Zheng et.al|[paper](https://arxiv.org/abs/2403.12505)|-|<details><summary>detail</summary>CVPR 2024</details>|\n", "GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning": "|**2024-3-21**|**GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning**|Sanqing Qu et.al|[paper](https://arxiv.org/abs/2403.14410)|-|<details><summary>detail</summary>This is a substantial extension of the CVPR 2023 paper \"Upcycling Models under Domain and Category Shift\"</details>|\n", "EventDance: Unsupervised Source-free Cross-modal Adaptation for Event-based Object Recognition": "|**2024-3-20**|**EventDance: Unsupervised Source-free Cross-modal Adaptation for Event-based Object Recognition**|Xu Zheng et.al|[paper](https://arxiv.org/abs/2403.14082)|-|<details><summary>detail</summary>CVPR2024</details>|\n", "When Cars meet Drones: Hyperbolic Federated Learning for Source-Free Domain Adaptation in Adverse Weather": "|**2024-3-20**|**When Cars meet Drones: Hyperbolic Federated Learning for Source-Free Domain Adaptation in Adverse Weather**|Giulia Rizzoli et.al|[paper](https://arxiv.org/abs/2403.13762)|-|-|\n", "Uncertainty-Aware Source-Free Adaptive Image Super-Resolution with Wavelet Augmentation Transformer": "|**2024-3-20**|**Uncertainty-Aware Source-Free Adaptive Image Super-Resolution with Wavelet Augmentation Transformer**|Yuang Ai et.al|[paper](https://arxiv.org/abs/2303.17783)|-|-|\n", "Source-Free Domain Adaptation for Question Answering with Masked Self-training": "|**2024-3-17**|**Source-Free Domain Adaptation for Question Answering with Masked Self-training**|M. Yin et.al|[paper](https://arxiv.org/abs/2212.09563)|-|-|\n", "Uncertainty-Aware Pseudo-Label Filtering for Source-Free Unsupervised Domain Adaptation": "|**2024-3-17**|**Uncertainty-Aware Pseudo-Label Filtering for Source-Free Unsupervised Domain Adaptation**|Xi Chen et.al|[paper](https://arxiv.org/abs/2403.11256)|[code](https://github.com/chenxi52/UPA.)|<details><summary>detail</summary>Neurocomputing 2024</details>|\n", "SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation": "|**2024-3-16**|**SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation**|Uiwon Hwang et.al|[paper](https://arxiv.org/abs/2403.10834)|[code](https://github.com/shinyflight/SFDA2)|<details><summary>detail</summary>ICLR 2024</details>|\n", "De-Confusing Pseudo-Labels in Source-Free Domain Adaptation": "|**2024-3-13**|**De-Confusing Pseudo-Labels in Source-Free Domain Adaptation**|Idit Diamant et.al|[paper](https://arxiv.org/abs/2401.01650)|-|-|\n", "Source-Free Domain Adaptation with Frozen Multimodal Foundation Model": "|**2024-3-13**|**Source-Free Domain Adaptation with Frozen Multimodal Foundation Model**|Song Tang et.al|[paper](https://arxiv.org/abs/2311.16510)|-|<details><summary>detail</summary>CVPR 2024</details>|\n", "Unified Source-Free Domain Adaptation": "|**2024-3-12**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|[code](https://github.com/tntek/source-free-domain-adaptation.)|-|\n", "COCA: Classifier-Oriented Calibration via Textual Prototype for Source-Free Universal Domain Adaptation": "|**2024-3-11**|**COCA: Classifier-Oriented Calibration via Textual Prototype for Source-Free Universal Domain Adaptation**|Xinghong Liu et.al|[paper](https://arxiv.org/abs/2308.10450)|-|-|\n", "Model-Free Source Seeking by a Novel Single-Integrator with Attenuating Oscillations and Better Convergence Rate: Robotic Experiments": "|**2024-3-8**|**Model-Free Source Seeking by a Novel Single-Integrator with Attenuating Oscillations and Better Convergence Rate: Robotic Experiments**|Shivam Bajpai et.al|[paper](https://arxiv.org/abs/2311.04330)|-|-|\n", "Agile Multi-Source-Free Domain Adaptation": "|**2024-3-8**|**Agile Multi-Source-Free Domain Adaptation**|Xinyao Li et.al|[paper](https://arxiv.org/abs/2403.05062)|[code](https://github.com/TL-UESTC/Bi-ATEN.)|<details><summary>detail</summary>AAAI2024</details>|\n", "MAP: MAsk-Pruning for Source-Free Model Intellectual Property Protection": "|**2024-3-6**|**MAP: MAsk-Pruning for Source-Free Model Intellectual Property Protection**|Boyang Peng et.al|[paper](https://arxiv.org/abs/2403.04149)|-|<details><summary>detail</summary>CVPR 2024</details>|\n"}, "object detection": {"Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection": "|**2024-3-22**|**Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection**|Hongzhi Gao et.al|[paper](https://arxiv.org/abs/2403.15317)|-|<details><summary>detail</summary>Accepted by AAAI2024</details>|\n", "IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection": "|**2024-3-22**|**IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection**|Junbo Yin et.al|[paper](https://arxiv.org/abs/2403.15241)|[code](https://github.com/yinjunbo/IS-Fusion.)|<details><summary>detail</summary>CVPR 2024</details>|\n", "CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations": "|**2024-3-22**|**CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations**|Yuwei Zhang et.al|[paper](https://arxiv.org/abs/2403.11220)|-|-|\n", "Gradient-based Sampling for Class Imbalanced Semi-supervised Object Detection": "|**2024-3-22**|**Gradient-based Sampling for Class Imbalanced Semi-supervised Object Detection**|Jiaming Li et.al|[paper](https://arxiv.org/abs/2403.15127)|[code](https://github.com/nightkeepers/CI-SSOD.)|<details><summary>detail</summary>Accepted by ICCV2023</details>|\n", "Rethinking Boundary Discontinuity Problem for Oriented Object Detection": "|**2024-3-21**|**Rethinking Boundary Discontinuity Problem for Oriented Object Detection**|Hang Xu et.al|[paper](https://arxiv.org/abs/2305.10061)|[code](https://github.com/hangxu-cv/cvpr24acm.)|<details><summary>detail</summary>cvpr 2024</details>|\n", "Online Open-set Semi-supervised Object Detection with Dual Competing Head": "|**2024-3-21**|**Online Open-set Semi-supervised Object Detection with Dual Competing Head**|Zerun Wang et.al|[paper](https://arxiv.org/abs/2305.13802)|-|-|\n", "T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy": "|**2024-3-21**|**T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy**|Qing Jiang et.al|[paper](https://arxiv.org/abs/2403.14610)|[code](https://github.com/IDEA-Research/T-Rex)|<details><summary>detail</summary>Technical Report</details>|\n", "Point2RBox: Combine Knowledge from Synthetic Visual Patterns for End-to-end Oriented Object Detection with Single Point Supervision": "|**2024-3-21**|**Point2RBox: Combine Knowledge from Synthetic Visual Patterns for End-to-end Oriented Object Detection with Single Point Supervision**|Yi Yu et.al|[paper](https://arxiv.org/abs/2311.14758)|[code](https://github.com/yuyi1005/point2rbox-mmrotate)|-|\n", "R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based Robots Ecosystems via Proposal Refinement": "|**2024-3-21**|**R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based Robots Ecosystems via Proposal Refinement**|Michele Antonazzi et.al|[paper](https://arxiv.org/abs/2403.11567)|-|-|\n", "3D Object Detection from Point Cloud via Voting Step Diffusion": "|**2024-3-21**|**3D Object Detection from Point Cloud via Voting Step Diffusion**|Haoran Hou et.al|[paper](https://arxiv.org/abs/2403.14133)|[code](https://github.com/HHrEtvP/DiffVote.)|-|\n", "Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban Environments": "|**2024-3-20**|**Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban Environments**|Djamahl Etchegaray et.al|[paper](https://arxiv.org/abs/2403.13556)|-|-|\n", "Few-shot Oriented Object Detection with Memorable Contrastive Learning in Remote Sensing Images": "|**2024-3-20**|**Few-shot Oriented Object Detection with Memorable Contrastive Learning in Remote Sensing Images**|Jiawei Zhou et.al|[paper](https://arxiv.org/abs/2403.13375)|-|-|\n", "D-YOLO a robust framework for object detection in adverse weather conditions": "|**2024-3-19**|**D-YOLO a robust framework for object detection in adverse weather conditions**|Zihan Chu et.al|[paper](https://arxiv.org/abs/2403.09233)|-|<details><summary>detail</summary>Object detection in adverse weather conditions</details>|\n", "BugNIST - a Large Volumetric Dataset for Object Detection under Domain Shift": "|**2024-3-19**|**BugNIST - a Large Volumetric Dataset for Object Detection under Domain Shift**|Patrick M\u00f8ller Jensen et.al|[paper](https://arxiv.org/abs/2304.01838)|-|-|\n", "Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector": "|**2024-3-19**|**Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector**|Yuqian Fu et.al|[paper](https://arxiv.org/abs/2402.03094)|-|-|\n"}, "domain adaptation": {"Improve Cross-domain Mixed Sampling with Guidance Training for Adaptive Segmentation": "|**2024-3-22**|**Improve Cross-domain Mixed Sampling with Guidance Training for Adaptive Segmentation**|Wenlve Zhou et.al|[paper](https://arxiv.org/abs/2403.14995)|[code](https://github.com/Wenlve-Zhou/Guidance-Training.)|-|\n", "CODA: A COst-efficient Test-time Domain Adaptation Mechanism for HAR": "|**2024-3-21**|**CODA: A COst-efficient Test-time Domain Adaptation Mechanism for HAR**|Minghui Qiu et.al|[paper](https://arxiv.org/abs/2403.14922)|-|-|\n", "DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning": "|**2024-3-21**|**DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning**|Jonathan Lebensold et.al|[paper](https://arxiv.org/abs/2403.14421)|-|-|\n", "GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning": "|**2024-3-21**|**GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning**|Sanqing Qu et.al|[paper](https://arxiv.org/abs/2403.14410)|-|<details><summary>detail</summary>This is a substantial extension of the CVPR 2023 paper \"Upcycling Models under Domain and Category Shift\"</details>|\n", "Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training": "|**2024-3-21**|**Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training**|Arun Reddy et.al|[paper](https://arxiv.org/abs/2312.02914)|-|<details><summary>detail</summary>CVPR 2024</details>|\n", "R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based Robots Ecosystems via Proposal Refinement": "|**2024-3-21**|**R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based Robots Ecosystems via Proposal Refinement**|Michele Antonazzi et.al|[paper](https://arxiv.org/abs/2403.11567)|-|-|\n", "A Fourier Transform Framework for Domain Adaptation": "|**2024-3-21**|**A Fourier Transform Framework for Domain Adaptation**|Le Luo et.al|[paper](https://arxiv.org/abs/2403.07798)|-|<details><summary>detail</summary>The paper contains significant errors and the experimental methodology is not rigorous</details>|\n", "Improving $\u039b$ Signal Extraction with Domain Adaptation via Normalizing Flows": "|**2024-3-20**|**Improving $\u039b$ Signal Extraction with Domain Adaptation via Normalizing Flows**|Rowan Kelleher et.al|[paper](https://arxiv.org/abs/2403.14076)|-|<details><summary>detail</summary>Proceedings for the 25th International Spin Physics Symposium (SPIN 2023)</details>|\n", "When Cars meet Drones: Hyperbolic Federated Learning for Source-Free Domain Adaptation in Adverse Weather": "|**2024-3-20**|**When Cars meet Drones: Hyperbolic Federated Learning for Source-Free Domain Adaptation in Adverse Weather**|Giulia Rizzoli et.al|[paper](https://arxiv.org/abs/2403.13762)|-|-|\n", "ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer": "|**2024-3-20**|**ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer**|Hiroki Azuma et.al|[paper](https://arxiv.org/abs/2403.13652)|-|-|\n", "High-confidence pseudo-labels for domain adaptation in COVID-19 detection": "|**2024-3-20**|**High-confidence pseudo-labels for domain adaptation in COVID-19 detection**|Robert Turnbull et.al|[paper](https://arxiv.org/abs/2403.13509)|-|-|\n", "Confusing Pair Correction Based on Category Prototype for Domain Adaptation under Noisy Environments": "|**2024-3-19**|**Confusing Pair Correction Based on Category Prototype for Domain Adaptation under Noisy Environments**|Churan Zhi et.al|[paper](https://arxiv.org/abs/2403.12883)|[code](https://github.com/Hehxcf/CPC/.)|<details><summary>detail</summary>AAAI 2024</details>|\n", "Addressing Source Scale Bias via Image Warping for Domain Adaptation": "|**2024-3-19**|**Addressing Source Scale Bias via Image Warping for Domain Adaptation**|Shen Zheng et.al|[paper](https://arxiv.org/abs/2403.12712)|-|-|\n", "Align and Distill: Unifying and Improving Domain Adaptive Object Detection": "|**2024-3-18**|**Align and Distill: Unifying and Improving Domain Adaptive Object Detection**|Justin Kay et.al|[paper](https://arxiv.org/abs/2403.12029)|[code](https://github.com/justinkay/aldi)|-|\n", "Optimal Transport for Domain Adaptation through Gaussian Mixture Models": "|**2024-3-18**|**Optimal Transport for Domain Adaptation through Gaussian Mixture Models**|Eduardo Fernandes Montesuma et.al|[paper](https://arxiv.org/abs/2403.13847)|-|-|\n"}, "domain generalization": {"Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics": "|**2024-3-21**|**Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics**|Jiaqi Yue et.al|[paper](https://arxiv.org/abs/2403.14362)|-|<details><summary>detail</summary>This work is submitted to IEEE TNNLS and is subject to IEEE copyright</details>|\n", "DomainLab: A modular Python package for domain generalization in deep learning": "|**2024-3-21**|**DomainLab: A modular Python package for domain generalization in deep learning**|Xudong Sun et.al|[paper](https://arxiv.org/abs/2403.14356)|[code](https://github.com/marrlab/DomainLab.)|-|\n", "V2X-DGW: Domain Generalization for Multi-agent Perception under Adverse Weather Conditions": "|**2024-3-20**|**V2X-DGW: Domain Generalization for Multi-agent Perception under Adverse Weather Conditions**|Baolu Li et.al|[paper](https://arxiv.org/abs/2403.11371)|-|-|\n", "A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation": "|**2024-3-19**|**A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation**|Qucheng Peng et.al|[paper](https://arxiv.org/abs/2403.11310)|[code](https://github.com/davidpengucf/DAF-DG)|<details><summary>detail</summary>Accepted by CVPR 2024</details>|\n", "Towards Generalizing to Unseen Domains with Few Labels": "|**2024-3-18**|**Towards Generalizing to Unseen Domains with Few Labels**|Chamuditha Jayanga Galappaththige et.al|[paper](https://arxiv.org/abs/2403.11674)|-|<details><summary>detail</summary>CVPR 2024</details>|\n", "SETA: Semantic-Aware Token Augmentation for Domain Generalization": "|**2024-3-18**|**SETA: Semantic-Aware Token Augmentation for Domain Generalization**|Jintao Guo et.al|[paper](https://arxiv.org/abs/2403.11792)|[code](https://github.com/lingeringlight/SETA.)|-|\n", "Learning General Policies for Classical Planning Domains: Getting Beyond C$_2$": "|**2024-3-18**|**Learning General Policies for Classical Planning Domains: Getting Beyond C$_2$**|Simon St\u00e5hlberg et.al|[paper](https://arxiv.org/abs/2403.11734)|-|<details><summary>detail</summary>Submitted to IJCAI 2024</details>|\n", "Depth- and Semantics-aware Multi-modal Domain Translation: Generating 3D Panoramic Color Images from LiDAR Point Clouds": "|**2024-3-18**|**Depth- and Semantics-aware Multi-modal Domain Translation: Generating 3D Panoramic Color Images from LiDAR Point Clouds**|Tiago Cortinhal et.al|[paper](https://arxiv.org/abs/2302.07661)|-|-|\n", "Understanding Domain Generalization: A Noise Robustness Perspective": "|**2024-3-17**|**Understanding Domain Generalization: A Noise Robustness Perspective**|Rui Qiao et.al|[paper](https://arxiv.org/abs/2401.14846)|[code](https://github.com/qiaoruiyt/NoiseRobustDG)|<details><summary>detail</summary>the 12th International Conference on Learning Representations (ICLR 2024)</details>|\n", "AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation": "|**2024-3-17**|**AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation**|Zihao Tang et.al|[paper](https://arxiv.org/abs/2403.07030)|[code](https://github.com/IshiKura-a/AuG-KD)|<details><summary>detail</summary>ICLR 2024</details>|\n", "Artifact Feature Purification for Cross-domain Detection of AI-generated Images": "|**2024-3-17**|**Artifact Feature Purification for Cross-domain Detection of AI-generated Images**|Zheling Meng et.al|[paper](https://arxiv.org/abs/2403.11172)|-|<details><summary>detail</summary>This work is under consideration at Computer Vision and Image Understanding</details>|\n", "Improving Domain Generalization with Domain Relations": "|**2024-3-16**|**Improving Domain Generalization with Domain Relations**|Huaxiu Yao et.al|[paper](https://arxiv.org/abs/2302.02609)|-|<details><summary>detail</summary>Accepted by ICLR 2024 (Spotlight)</details>|\n", "TFS-ViT: Token-Level Feature Stylization for Domain Generalization": "|**2024-3-16**|**TFS-ViT: Token-Level Feature Stylization for Domain Generalization**|Mehrdad Noori et.al|[paper](https://arxiv.org/abs/2303.15698)|[code](https://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization.)|-|\n", "Bidirectional Multi-Step Domain Generalization for Visible-Infrared Person Re-Identification": "|**2024-3-15**|**Bidirectional Multi-Step Domain Generalization for Visible-Infrared Person Re-Identification**|Mahdi Alehdaghi et.al|[paper](https://arxiv.org/abs/2403.10782)|-|-|\n", "Seeking Flat Minima with Mean Teacher on Semi- and Weakly-Supervised Domain Generalization for Object Detection": "|**2024-3-15**|**Seeking Flat Minima with Mean Teacher on Semi- and Weakly-Supervised Domain Generalization for Object Detection**|Ryosuke Furuta et.al|[paper](https://arxiv.org/abs/2310.19351)|-|-|\n"}, "vision language": {"Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images": "|**2024-3-22**|**Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images**|Kuofeng Gao et.al|[paper](https://arxiv.org/abs/2401.11170)|[code](https://github.com/KuofengGao/Verbose_Images.)|<details><summary>detail</summary>Accepted by ICLR 2024</details>|\n", "Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning": "|**2024-3-22**|**Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning**|Yunhao Gou et.al|[paper](https://arxiv.org/abs/2312.12379)|[code](https://gyhdog99.github.io/projects/mocle/)|<details><summary>detail</summary>Project website: https://gyhdog99</details>|\n", "Continual Vision-and-Language Navigation": "|**2024-3-22**|**Continual Vision-and-Language Navigation**|Seongjun Jeong et.al|[paper](https://arxiv.org/abs/2403.15049)|-|-|\n", "Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization": "|**2024-3-22**|**Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization**|Yang Jin et.al|[paper](https://arxiv.org/abs/2309.04669)|[code](https://github.com/jy0205/LaVIT.)|<details><summary>detail</summary>ICLR 2024</details>|\n", "Few-Shot Adversarial Prompt Learning on Vision-Language Models": "|**2024-3-21**|**Few-Shot Adversarial Prompt Learning on Vision-Language Models**|Yiwei Zhou et.al|[paper](https://arxiv.org/abs/2403.14774)|-|-|\n", "Can 3D Vision-Language Models Truly Understand Natural Language?": "|**2024-3-21**|**Can 3D Vision-Language Models Truly Understand Natural Language?**|Weipeng Deng et.al|[paper](https://arxiv.org/abs/2403.14760)|[code](https://github.com/VincentDENGP/3D-LR)|<details><summary>detail</summary>https://github</details>|\n", "Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model": "|**2024-3-21**|**Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model**|Hao Cheng et.al|[paper](https://arxiv.org/abs/2402.19150)|-|-|\n", "Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models": "|**2024-3-21**|**Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models**|Zuyan Liu et.al|[paper](https://arxiv.org/abs/2403.12966)|[code](https://github.com/dongyh20/Chain-of-Spot)|<details><summary>detail</summary>Project Page: https://sites</details>|\n", "Active Prompt Learning in Vision Language Models": "|**2024-3-21**|**Active Prompt Learning in Vision Language Models**|Jihwan Bang et.al|[paper](https://arxiv.org/abs/2311.11178)|[code](https://github.com/kaist-dmlab/pcb)|<details><summary>detail</summary>accepted at CVPR 2024</details>|\n", "Volumetric Environment Representation for Vision-Language Navigation": "|**2024-3-21**|**Volumetric Environment Representation for Vision-Language Navigation**|Rui Liu et.al|[paper](https://arxiv.org/abs/2403.14158)|-|<details><summary>detail</summary>CVPR 2024</details>|\n", "Vision-Language Models can Identify Distracted Driver Behavior from Naturalistic Videos": "|**2024-3-21**|**Vision-Language Models can Identify Distracted Driver Behavior from Naturalistic Videos**|Md Zahid Hasan et.al|[paper](https://arxiv.org/abs/2306.10159)|-|-|\n", "C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion": "|**2024-3-21**|**C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion**|Hee Suk Yoon et.al|[paper](https://arxiv.org/abs/2403.14119)|-|<details><summary>detail</summary>ICLR 2024</details>|\n", "Bridge the Modality and Capacity Gaps in Vision-Language Model Selection": "|**2024-3-20**|**Bridge the Modality and Capacity Gaps in Vision-Language Model Selection**|Chao Yi et.al|[paper](https://arxiv.org/abs/2403.13797)|-|-|\n", "Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models": "|**2024-3-20**|**Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models**|Nicholas Bai et.al|[paper](https://arxiv.org/abs/2403.13771)|-|-|\n", "Enhancing Gait Video Analysis in Neurodegenerative Diseases by Knowledge Augmentation in Vision Language Model": "|**2024-3-20**|**Enhancing Gait Video Analysis in Neurodegenerative Diseases by Knowledge Augmentation in Vision Language Model**|Diwei Wang et.al|[paper](https://arxiv.org/abs/2403.13756)|-|-|\n"}}